# Prompt Injection Study Plan

## 1. Core Concepts (1-2 Weeks)
- [ ] **Introduction to Prompt Injection**
  - Define prompt injection in the context of large language models (LLMs).
  - Understand the differences between prompt injection, chain-of-thought manipulation, and traditional adversarial attacks.
- [ ] **Common Challenges in Prompt Injection**
  - Identify vulnerabilities in prompt design and execution.
  - Explore potential risks in real-world AI deployments due to malicious prompt manipulation.

## 2. Prompt Injection Techniques (2-3 Weeks)
- [ ] **Direct Prompt Injection**
  - Learn how adversaries insert malicious instructions directly into input prompts.
- [ ] **Indirect Prompt Injection**
  - Study how contextual or background data can be exploited to alter LLM behavior.
- [ ] **Chain-of-Thought Manipulation**
  - Understand methods that influence the reasoning process of LLMs.
- [ ] **Case Studies and Examples**
  - Analyze documented instances of prompt injection attacks in LLMs.

## 3. Defense and Mitigation Strategies (2-3 Weeks)
- [ ] **Robust Prompt Engineering**
  - Explore techniques for designing secure and robust prompts.
- [ ] **Input Sanitization and Filtering**
  - Learn methods to detect and neutralize injected content.
- [ ] **Evaluation and Metrics**
  - Establish metrics to assess the effectiveness of mitigation strategies.
- [ ] **Best Practices**
  - Compile guidelines for reducing susceptibility to prompt injection.

---

## Study Resources for Prompt Injection

### Key Papers
- **Automatic and Universal Prompt Injection Attacks against Large Language Models**  
  [arXiv Search Results](https://arxiv.org/abs/2403.04957)
- **LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models**  
  [arXiv Search Results](https://arxiv.org/abs/2403.16432)

### ToolBox
- **Chatgpt_System_Prompt**  
  A collection of GPT system prompts and various prompt injection/leaking knowledge.
  [GitHub - Chatgpt_System_Prompt](https://github.com/LouisShark/chatgpt_system_prompt)  



### Awesome Repos
- **Awesome LLM Injection**  
  ChatGPT Jailbreaks, GPT Assistants Prompt Leaks, GPTs Prompt Injection, LLM Prompt Security, Super Prompts, Prompt Hack, Prompt Security, Ai Prompt Engineering, Adversarial Machine Learning.
  
  [GitHub - Awesome_GPT_Super_Prompting](https://github.com/CyberAlbSecOP/Awesome_GPT_Super_Prompting)
